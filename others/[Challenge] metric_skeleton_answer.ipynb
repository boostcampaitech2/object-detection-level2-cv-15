{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"colab":{"name":"metric_challenge.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"8c7dbb27"},"source":["!pip install tqdm\n","!pip install pycocotools"],"id":"8c7dbb27","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0c243d84"},"source":["import pandas as pd\n","import numpy as np\n","import json\n","from tqdm import tqdm\n","from pycocotools.coco import COCO"],"id":"0c243d84","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"239e1778"},"source":["GT_JSON = '../../dataset/train.json'\n","PRED_CSV = 'put submission csv'\n","\n","\n","    \n","# load ground truth\n","with open(GT_JSON, 'r') as outfile:\n","    test_anno = (json.load(outfile))\n","\n","# load prediction\n","pred_df = pd.read_csv(PRED_CSV)\n","\n","   "],"id":"239e1778","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"78d556fc"},"source":["'''\n","[\n","    [file_name 1, confidence_score, x_min, x_max, y_min, y_max], \n","    [file_name 2 confidence_score, x_min, x_max, y_min, y_max],\n","    ,,,\n","    [file_name , confidence_score, x_min, x_max, y_min, y_max]\n","]\n","'''\n","    \n","new_pred = []\n","\n","file_names = pred_df['image_id'].values.tolist()\n","bboxes = pred_df['PredictionString'].values.tolist()\n","    \n","'''\n","create new_pred\n","'''\n","    \n","for i, bbox in enumerate(bboxes):\n","    if isinstance(bbox, float):\n","        print(f'{file_names[i]} empty box')\n","\n","for file_name, bbox in tqdm(zip(file_names, bboxes)):\n","    boxes = np.array(str(bbox).split(' '))\n","    \n","    if len(boxes) % 6 == 1:\n","        boxes = boxes[:-1].reshape(-1, 6)\n","    elif len(boxes) % 6 == 0:\n","        boxes = boxes.reshape(-1, 6)\n","    else:\n","        raise Exception('error', 'invalid box count')\n","    for box in boxes:\n","        new_pred.append([file_name, box[0], box[1], float(box[2]), float(box[4]), float(box[3]), float(box[5])])"],"id":"78d556fc","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"12e02aee"},"source":["'''\n","[\n","    [file_name 1, confidence_score, x_min, x_max, y_min, y_max], \n","    [file_name 2, confidence_score, x_min, x_max, y_min, y_max],\n","    ,,,\n","    [file_name , confidence_score, x_min, x_max, y_min, y_max]\n","]\n","'''\n","    \n","gt = []\n","\n","'''\n","create gt\n","'''\n","    \n","coco = COCO(GT_JSON)\n","   \n","'''\n","coco.getImgIds(): return image id list\n","    \n","coco.loadImgs(image_id): return image_info\n","    \n","image_info['file_name']: return file name\n","   \n","coco.getAnnIds(imgIds=image_info['id']): return annotation id\n","    \n","coco.loadAnns(ann_ids): return annotation information list (annotation_info_list)\n","    \n","annotation_info_list[i]['bbox']: return i'th annotation [x_min, y_min, w, h]\n","    \n","annotation_info_list[i]['category_id']: return i'th annotation category\n","    \n","'''\n","    \n","for image_id in coco.getImgIds():\n","        \n","    image_info = coco.loadImgs(image_id)[0]\n","    annotation_id = coco.getAnnIds(imgIds=image_info['id'])\n","    annotation_info_list = coco.loadAnns(annotation_id)\n","        \n","    file_name = image_info['file_name']\n","        \n","    for annotation in annotation_info_list:\n","        gt.append([file_name, annotation['category_id'],\n","                   float(annotation['bbox'][0]),\n","                   float(annotation['bbox'][0]) + float(annotation['bbox'][2]),\n","                   float(annotation['bbox'][1]),\n","                   (float(annotation['bbox'][1]) + float(annotation['bbox'][3]))])"],"id":"12e02aee","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5f828493"},"source":["def compute_overlap(boxes, query_boxes):\n","    \"\"\"\n","    Args\n","        a: (N, 4) ndarray of float\n","        b: (K, 4) ndarray of float\n","    Returns\n","        overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n","    \"\"\"\n","    N = boxes.shape[0]\n","    K = query_boxes.shape[0]\n","    overlaps = np.zeros((N, K), dtype=np.float64)\n","    for k in range(K):\n","        box_area = (\n","            (query_boxes[k, 2] - query_boxes[k, 0]) *\n","            (query_boxes[k, 3] - query_boxes[k, 1])\n","        )\n","        for n in range(N):\n","            iw = (\n","                min(boxes[n, 2], query_boxes[k, 2]) -\n","                max(boxes[n, 0], query_boxes[k, 0])\n","            )\n","            if iw > 0:\n","                ih = (\n","                    min(boxes[n, 3], query_boxes[k, 3]) -\n","                    max(boxes[n, 1], query_boxes[k, 1])\n","                )\n","                if ih > 0:\n","                    ua = np.float64(\n","                        (boxes[n, 2] - boxes[n, 0]) *\n","                        (boxes[n, 3] - boxes[n, 1]) +\n","                        box_area - iw * ih\n","                    )\n","                    overlaps[n, k] = iw * ih / ua\n","    return overlaps"],"id":"5f828493","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"adbb4699"},"source":["def get_real_annotations(table):\n","    res = dict()\n","    ids = table['ImageID'].values.astype(np.str)\n","    labels = table['LabelName'].values.astype(np.str)\n","    xmin = table['XMin'].values.astype(np.float32)\n","    xmax = table['XMax'].values.astype(np.float32)\n","    ymin = table['YMin'].values.astype(np.float32)\n","    ymax = table['YMax'].values.astype(np.float32)\n","\n","    for i in range(len(ids)):\n","        id = ids[i]\n","        label = labels[i]\n","        if id not in res:\n","            res[id] = dict()\n","        if label not in res[id]:\n","            res[id][label] = []\n","        box = [xmin[i], ymin[i], xmax[i], ymax[i]]\n","        res[id][label].append(box)\n","\n","    return res\n","\n","\n","def get_detections(table):\n","    res = dict()\n","    ids = table['ImageID'].values.astype(np.str)\n","    labels = table['LabelName'].values.astype(np.str)\n","    scores = table['Conf'].values.astype(np.float32)\n","    xmin = table['XMin'].values.astype(np.float32)\n","    xmax = table['XMax'].values.astype(np.float32)\n","    ymin = table['YMin'].values.astype(np.float32)\n","    ymax = table['YMax'].values.astype(np.float32)\n","\n","    for i in range(len(ids)):\n","        id = ids[i]\n","        label = labels[i]\n","        if id not in res:\n","            res[id] = dict()\n","        if label not in res[id]:\n","            res[id][label] = []\n","        box = [xmin[i], ymin[i], xmax[i], ymax[i], scores[i]]\n","        res[id][label].append(box)\n","\n","    return res\n","\n","\n","def _compute_ap(recall, precision):\n","    \"\"\" Compute the average precision, given the recall and precision curves.\n","    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n","    # Arguments\n","        recall:    The recall curve (list).\n","        precision: The precision curve (list).\n","    # Returns\n","        The average precision as computed in py-faster-rcnn.\n","    \"\"\"\n","    # correct AP calculation\n","    # first append sentinel values at the end\n","    mrec = np.concatenate(([0.], recall, [1.]))\n","    mpre = np.concatenate(([0.], precision, [0.]))\n","\n","    # compute the precision envelope\n","    for i in range(mpre.size - 1, 0, -1):\n","        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n","\n","    # to calculate area under PR curve, look for points\n","    # where X axis (recall) changes value\n","    i = np.where(mrec[1:] != mrec[:-1])[0]\n","\n","    # and sum (\\Delta recall) * prec\n","    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n","    return ap"],"id":"adbb4699","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b3ae4b69"},"source":["def mean_average_precision_for_boxes(ann, pred, iou_threshold=0.5):\n","    \"\"\"\n","    :param ann: path to CSV-file with annotations or numpy array of shape (N, 6)\n","    :param pred: path to CSV-file with predictions (detections) or numpy array of shape (N, 7)\n","    :param iou_threshold: IoU between boxes which count as 'match'. Default: 0.5\n","    :return: tuple, where first value is mAP and second values is dict with AP for each class.\n","    \"\"\"\n","\n","    if isinstance(ann, str):\n","        valid = pd.read_csv(ann)\n","    else:\n","        valid = pd.DataFrame(ann, columns=['ImageID', 'LabelName', 'XMin', 'XMax', 'YMin', 'YMax'])\n","\n","    if isinstance(pred, str):\n","        preds = pd.read_csv(pred)\n","    else:\n","        preds = pd.DataFrame(pred, columns=['ImageID', 'LabelName', 'Conf', 'XMin', 'XMax', 'YMin', 'YMax'])\n","\n","    ann_unique = valid['ImageID'].unique()\n","    preds_unique = preds['ImageID'].unique()\n","\n","    print('Number of files in annotations: {}'.format(len(ann_unique)))\n","    print('Number of files in predictions: {}'.format(len(preds_unique)))\n","\n","\n","    unique_classes = valid['LabelName'].unique().astype(np.str)\n","    \n","    print('Unique classes: {}'.format(len(unique_classes)))\n","\n","    all_detections = get_detections(preds)\n","    all_annotations = get_real_annotations(valid)\n","    \n","    print('Detections length: {}'.format(len(all_detections)))\n","    print('Annotations length: {}'.format(len(all_annotations)))\n","\n","    average_precisions = {}\n","    for zz, label in enumerate(sorted(unique_classes)):\n","\n","        # Negative class\n","        if str(label) == 'nan':\n","            continue\n","\n","        false_positives = []\n","        true_positives = []\n","        scores = []\n","        num_annotations = 0.0\n","\n","        for i in range(len(ann_unique)):\n","            detections = []\n","            annotations = []\n","            id = ann_unique[i]\n","            if id in all_detections:\n","                if label in all_detections[id]:\n","                    detections = all_detections[id][label]\n","            if id in all_annotations:\n","                if label in all_annotations[id]:\n","                    annotations = all_annotations[id][label]\n","\n","            if len(detections) == 0 and len(annotations) == 0:\n","                continue\n","\n","            num_annotations += len(annotations)\n","            detected_annotations = []\n","\n","            annotations = np.array(annotations, dtype=np.float64)\n","            for d in detections:\n","                scores.append(d[4])\n","\n","                if len(annotations) == 0:\n","                    false_positives.append(1)\n","                    true_positives.append(0)\n","                    continue\n","\n","                overlaps = compute_overlap(np.expand_dims(np.array(d, dtype=np.float64), axis=0), annotations)\n","                assigned_annotation = np.argmax(overlaps, axis=1)\n","                max_overlap = overlaps[0, assigned_annotation]\n","\n","                if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations:\n","                    false_positives.append(0)\n","                    true_positives.append(1)\n","                    detected_annotations.append(assigned_annotation)\n","                else:\n","                    false_positives.append(1)\n","                    true_positives.append(0)\n","\n","        if num_annotations == 0:\n","            average_precisions[label] = 0, 0\n","            continue\n","\n","        false_positives = np.array(false_positives)\n","        true_positives = np.array(true_positives)\n","        scores = np.array(scores)\n","\n","        # sort by score\n","        indices = np.argsort(-scores)\n","        false_positives = false_positives[indices]\n","        true_positives = true_positives[indices]\n","\n","        # compute false positives and true positives\n","        false_positives = np.cumsum(false_positives)\n","        true_positives = np.cumsum(true_positives)\n","\n","        # compute recall and precision\n","        recall = true_positives / num_annotations\n","        precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n","\n","        # compute average precision\n","        average_precision = _compute_ap(recall, precision)\n","        average_precisions[label] = average_precision, num_annotations\n","        s1 = \"{:30s} | {:.6f} | {:7d}\".format(label, average_precision, int(num_annotations))\n","        print(s1)\n","\n","    present_classes = 0\n","    precision = 0\n","    for label, (average_precision, num_annotations) in average_precisions.items():\n","        if num_annotations > 0:\n","            present_classes += 1\n","            precision += average_precision\n","            \n","    mean_ap = precision / present_classes\n","    print('mAP: {:.6f}'.format(mean_ap))\n","    \n","    return mean_ap, average_precisions"],"id":"b3ae4b69","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a0938f82"},"source":["'''\n","calculate mAP\n","'''\n","\n","mean_ap, average_precisions = mean_average_precision_for_boxes(gt, new_pred, iou_threshold=0.5)\n","\n","print(mean_ap)"],"id":"a0938f82","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ef39d7ef"},"source":[""],"id":"ef39d7ef","execution_count":null,"outputs":[]}]}