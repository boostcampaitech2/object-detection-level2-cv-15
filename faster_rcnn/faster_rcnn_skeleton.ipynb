{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Library Import"
   ],
   "metadata": {
    "id": "xsgTBr3BqE8z"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import os\n",
    "import six\n",
    "from collections import namedtuple\n",
    "\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.ops import RoIPool\n",
    "from torchvision.ops import nms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils import data as data_\n",
    "\n",
    "from torchnet.meter import ConfusionMeter, AverageValueMeter"
   ],
   "outputs": [],
   "metadata": {
    "id": "c142ed1a"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Util Functions"
   ],
   "metadata": {
    "id": "2wb--qP79Xx2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def loc2bbox(src_bbox, loc):\n",
    "    \"\"\"\n",
    "    from src_bbox to dst bbox using loc\n",
    "    Args:\n",
    "        src_bbox: 소스 바운딩 박스\n",
    "        loc: 델타\n",
    "    Returns: dst_bbox\n",
    "    \"\"\"\n",
    "\n",
    "    if src_bbox.shape[0] == 0:\n",
    "        return np.zeros((0, 4), dtype=loc.dtype)\n",
    "\n",
    "    src_bbox = src_bbox.astype(src_bbox.dtype, copy=False)\n",
    "\n",
    "    # x_min, y_min, x_max, y_max\n",
    "    src_height = src_bbox[:, 2] - src_bbox[:, 0]\n",
    "    src_width = src_bbox[:, 3] - src_bbox[:, 1]\n",
    "    src_ctr_y = src_bbox[:, 0] + 0.5 * src_height\n",
    "    src_ctr_x = src_bbox[:, 1] + 0.5 * src_width\n",
    "\n",
    "    dy = loc[:, 0::4]\n",
    "    dx = loc[:, 1::4]\n",
    "    dh = loc[:, 2::4]\n",
    "    dw = loc[:, 3::4]\n",
    "\n",
    "    ctr_y = dy * src_height[:, np.newaxis] + src_ctr_y[:, np.newaxis]\n",
    "    ctr_x = dx * src_width[:, np.newaxis] + src_ctr_x[:, np.newaxis]\n",
    "    h = np.exp(dh) * src_height[:, np.newaxis]\n",
    "    w = np.exp(dw) * src_width[:, np.newaxis]\n",
    "\n",
    "    dst_bbox = np.zeros(loc.shape, dtype=loc.dtype)\n",
    "    dst_bbox[:, 0::4] = ctr_y - 0.5 * h\n",
    "    dst_bbox[:, 1::4] = ctr_x - 0.5 * w\n",
    "    dst_bbox[:, 2::4] = ctr_y + 0.5 * h\n",
    "    dst_bbox[:, 3::4] = ctr_x + 0.5 * w\n",
    "\n",
    "    return dst_bbox\n",
    "\n",
    "\n",
    "def bbox2loc(src_bbox, dst_bbox):\n",
    "    \"\"\"\n",
    "    src_bbox : 예측된 좌표값(or anchor), dst_bbox: gt 좌표값 -> loc(y, x, h, w)\n",
    "    \"\"\"\n",
    "\n",
    "    # x_min, y_min, x_max, y_max\n",
    "    height = src_bbox[:, 2] - src_bbox[:, 0]\n",
    "    width = src_bbox[:, 3] - src_bbox[:, 1]\n",
    "    ctr_y = src_bbox[:, 0] + 0.5 * height\n",
    "    ctr_x = src_bbox[:, 1] + 0.5 * width\n",
    "\n",
    "    # x_min, y_min, x_max, y_max\n",
    "    base_height = dst_bbox[:, 2] - dst_bbox[:, 0]\n",
    "    base_width = dst_bbox[:, 3] - dst_bbox[:, 1]\n",
    "    base_ctr_y = dst_bbox[:, 0] + 0.5 * base_height\n",
    "    base_ctr_x = dst_bbox[:, 1] + 0.5 * base_width\n",
    "\n",
    "    eps = np.finfo(height.dtype).eps\n",
    "    height = np.maximum(height, eps)\n",
    "    width = np.maximum(width, eps)\n",
    "\n",
    "    dy = (base_ctr_y - ctr_y) / height\n",
    "    dx = (base_ctr_x - ctr_x) / width\n",
    "    dh = np.log(base_height / height)\n",
    "    dw = np.log(base_width / width)\n",
    "\n",
    "    loc = np.vstack((dy, dx, dh, dw)).transpose()\n",
    "    return loc\n",
    "\n",
    "\n",
    "def normal_init(m, mean, stddev, truncated=False):\n",
    "    \"\"\"\n",
    "    weight initialization\n",
    "    \"\"\"\n",
    "    if truncated:\n",
    "        m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean) \n",
    "    else:\n",
    "        m.weight.data.normal_(mean, stddev)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def get_inside_index(anchor, H, W):\n",
    "    # Calc indicies of anchors which are located completely inside of the image\n",
    "    # whose size is speficied.\n",
    "    index_inside = np.where(\n",
    "        (anchor[:, 0] >= 0) &\n",
    "        (anchor[:, 1] >= 0) &\n",
    "        (anchor[:, 2] <= H) &\n",
    "        (anchor[:, 3] <= W)\n",
    "    )[0]\n",
    "    return index_inside\n",
    "\n",
    "\n",
    "def unmap(data, count, index, fill=0):\n",
    "    # Unmap a subset of item (data) back to the original set of items (of size count)\n",
    "    if len(data.shape) == 1:\n",
    "        ret = np.empty((count,), dtype=data.dtype)\n",
    "        ret.fill(fill)\n",
    "        ret[index] = data\n",
    "    else:\n",
    "        ret = np.empty((count,) + data.shape[1:], dtype=data.dtype)\n",
    "        ret.fill(fill)\n",
    "        ret[index, :] = data\n",
    "    return ret\n",
    "\n",
    "\n",
    "## util ##\n",
    "def tonumpy(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return data\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.detach().cpu().numpy()\n",
    "\n",
    "def totensor(data, cuda = True):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        tensor = torch.from_numpy(data)\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        tensor = data.detach()\n",
    "    if cuda:\n",
    "        tensor = tensor.cuda()\n",
    "    return tensor\n",
    "\n",
    "def scalar(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return data.reshape(1)[0]\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.item()"
   ],
   "outputs": [],
   "metadata": {
    "id": "1DTpLq7_9Qr2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main"
   ],
   "metadata": {
    "id": "PJhVDlP3qA7h"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 하이퍼 파라미터 세팅"
   ],
   "metadata": {
    "id": "e0ecf14d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "epochs= 12\n",
    "learning_rate = 0.005\n",
    "lr_decay = 0.1\n",
    "weight_decay = 0.0005\n",
    "use_drop = True   # use dropout in RoIHead\n",
    "\n",
    "rpn_sigma = 3.     # sigma for l1_smooth_loss (RPN loss)\n",
    "roi_sigma = 1.     # sigma for l1_smooth_loss (ROI loss)\n",
    "\n",
    "data_dir = '../dataset'   # 데이터 경로 \n",
    "train_load_path = None  # train시 checkpoint 경로\n",
    "inf_load_path = './checkpoints/faster_rcnn_scratch_checkpoints.pth' # inference시 체크포인트 경로"
   ],
   "outputs": [],
   "metadata": {
    "id": "166d1663"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset 만들기"
   ],
   "metadata": {
    "id": "49fb9a7d"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. custom data 불러 오기"
   ],
   "metadata": {
    "id": "a093db22"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# TrainDataset\n",
    "class TrainCustom(Dataset):\n",
    "    def __init__(self, annotation, data_dir, transforms = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotation: annotation 파일 위치\n",
    "            data_dir: data가 존재하는 폴더 경로\n",
    "            transforms : transform or not\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        # coco annotation 불러오기 (coco API)\n",
    "        self.coco = COCO(annotation)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        # 이미지 아이디 가져오기\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "\n",
    "        # 이미지 정보 가져오기\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "\n",
    "        # 이미지 로드\n",
    "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        # 어노테이션 파일 로드\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        # 박스 가져오기\n",
    "        boxes = np.array([x['bbox'] for x in anns])\n",
    "\n",
    "        # boxes (x_min, y_min, x_max, y_max)\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "\n",
    "        # 레이블 가져오기\n",
    "        labels = np.array([x['category_id'] for x in anns])\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        # transform 함수 정의\n",
    "        if self.transforms :\n",
    "            scale = 1.0  # resize scale\n",
    "            H, W, _ = image.shape\n",
    "            resize_H = int(scale * H)\n",
    "            resize_W = int(scale * W)\n",
    "            transforms = get_train_transform(resize_H, resize_W)\n",
    "        else :\n",
    "            scale = 1.0\n",
    "            transforms = no_transform()\n",
    "        \n",
    "        # transform\n",
    "        sample = {\n",
    "            'image': image,\n",
    "            'bboxes': boxes,\n",
    "            'labels': labels\n",
    "        }\n",
    "        sample = transforms(**sample)\n",
    "        image = sample['image']\n",
    "        bboxes = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
    "        boxes = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
    "\n",
    "        # bboxes (x_min, y_min, x_max, y_max) -> boxes (y_min, x_min, y_max, x_max)\n",
    "        boxes[:, 0] = bboxes[:, 1]\n",
    "        boxes[:, 1] = bboxes[:, 0]\n",
    "        boxes[:, 2] = bboxes[:, 3]\n",
    "        boxes[:, 3] = bboxes[:, 2]\n",
    "\n",
    "        return image, boxes, labels, scale\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.coco.getImgIds())\n",
    "\n",
    "# Test Datset\n",
    "class TestCustom(Dataset):\n",
    "    def __init__(self, annotation, data_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotation: annotation 파일 위치\n",
    "            data_dir: data가 존재하는 폴더 경로\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        # coco annotation 불러오기 (coco API)\n",
    "        self.coco = COCO(annotation)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        # 이미지 아이디 가져오기\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "\n",
    "        # 이미지 정보 가져오기\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "\n",
    "        # 이미지 로드\n",
    "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        image = torch.tensor(image, dtype = torch.float).permute(2,0,1)\n",
    "        \n",
    "        return image, image.shape[1:]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.coco.getImgIds())\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "6e08cf31"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Transform"
   ],
   "metadata": {
    "id": "bc847ee7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Train dataset transform\n",
    "def get_train_transform(h, w):\n",
    "    return A.Compose([\n",
    "        A.Resize(height = h, width = w),\n",
    "        A.Flip(p=0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "# No transform\n",
    "def no_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0) # format for pytorch tensor\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ],
   "outputs": [],
   "metadata": {
    "id": "6d2af672"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RPN (Region Proposal Network) 정의\n"
   ],
   "metadata": {
    "id": "e8a02619"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Anchor box 생성 (generate_anchor_base)"
   ],
   "metadata": {
    "id": "43368909"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def generate_anchor_base(base_size=16, ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32]):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        ratios: 비율\n",
    "        anchor_scales: 스케일\n",
    "    Returns: basic anchor boxes, shape=(R, 4)\n",
    "        R: len(ratio) * len(anchor_scales) = anchor 개수 = 9\n",
    "        4: anchor box 좌표 값\n",
    "    \"\"\"\n",
    "\n",
    "    py = base_size / 2. # center y\n",
    "    px = base_size / 2. # center x\n",
    "\n",
    "    anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4), dtype=np.float32) # anchor_box\n",
    "    \n",
    "    for i in six.moves.range(len(ratios)):\n",
    "        for j in six.moves.range(len(anchor_scales)):\n",
    "            h = base_size * anchor_scales[j] * np.sqrt(ratios[i])\n",
    "            w = base_size * anchor_scales[j] * np.sqrt(1. / ratios[i])\n",
    "\n",
    "            index = i * len(anchor_scales) + j\n",
    "            # offset of anchor box\n",
    "            anchor_base[index, 0] = py - h / 2. # y_min\n",
    "            anchor_base[index, 1] = px - w / 2. # x_min\n",
    "            anchor_base[index, 2] = py + h / 2. # y_max\n",
    "            anchor_base[index, 3] = px + w / 2. # x_max\n",
    "            \n",
    "    return anchor_base # (9,4)"
   ],
   "outputs": [],
   "metadata": {
    "id": "2155d0ec"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Proposal 생성 (ProposalCreator)\n",
    "RPN에서 구한 rpn_loc와 anchor을 통해서 Region of Interest(RoI)를 생성\\\n",
    "RoI 개수 줄이기 위해서 미리 정해둔 크기(min_size)에 맞는 roi들 중 NMS를 통해 최종 RoI 반환 (train 시 2000개)"
   ],
   "metadata": {
    "id": "e74ef20d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class ProposalCreator:\n",
    "    def __init__(self, parent_model,\n",
    "                 nms_thresh=0.7, # nms threshold\n",
    "                 n_train_pre_nms=12000, # train시 nms 전 roi 개수\n",
    "                 n_train_post_nms=2000, # train시 nms 후 roi 개수\n",
    "                 n_test_pre_nms=6000,   # test시 nms 전 roi 개수\n",
    "                 n_test_post_nms=300,   # test시 nms 후 roi 개수\n",
    "                 min_size=16            \n",
    "                 ):\n",
    "        self.parent_model = parent_model # 해당 모델이 train중인지 test중인지 나타냄\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.n_train_pre_nms = n_train_pre_nms\n",
    "        self.n_train_post_nms = n_train_post_nms\n",
    "        self.n_test_pre_nms = n_test_pre_nms\n",
    "        self.n_test_post_nms = n_test_post_nms\n",
    "        self.min_size = min_size\n",
    "\n",
    "    def __call__(self, loc, score, anchor, img_size, scale=1.):    \n",
    "        if self.parent_model.training: # train중일 때\n",
    "            n_pre_nms = self.n_train_pre_nms\n",
    "            n_post_nms = self.n_train_post_nms\n",
    "        else: # test중일 때\n",
    "            n_pre_nms = self.n_test_pre_nms\n",
    "            n_post_nms = self.n_test_post_nms\n",
    "\n",
    "        roi = loc2bbox(anchor, loc) # anchor의 좌표값과 predicted bounding bounding box offset(y,x,h,w)를 통해 bounding box 좌표값(y_min, x_min, y_max, x_max) 생성\n",
    "\n",
    "        # Clip predicted boxes to image.\n",
    "        roi[:, slice(0, 4, 2)] = np.clip(roi[:, slice(0, 4, 2)], 0, img_size[0])\n",
    "        roi[:, slice(1, 4, 2)] = np.clip(roi[:, slice(1, 4, 2)], 0, img_size[1])\n",
    "\n",
    "        # min_size 보다 작은 box들은 제거\n",
    "        min_size = self.min_size * scale\n",
    "        hs = roi[:, 2] - roi[:, 0]\n",
    "        ws = roi[:, 3] - roi[:, 1]\n",
    "        keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n",
    "        roi = roi[keep, :]\n",
    "        score = score[keep]\n",
    "        \n",
    "        # Sort all (proposal, score) pairs by score from highest to lowest.\n",
    "        # Take top pre_nms_topN \n",
    "        order = score.ravel().argsort()[::-1]\n",
    "        if n_pre_nms > 0:\n",
    "            order = order[:n_pre_nms]\n",
    "        roi = roi[order, :]\n",
    "        score = score[order]\n",
    "\n",
    "        # nms 적용\n",
    "        keep = nms(\n",
    "            torch.from_numpy(roi).cuda(),\n",
    "            torch.from_numpy(score).cuda(),\n",
    "            self.nms_thresh)\n",
    "        if n_post_nms > 0:\n",
    "            keep = keep[:n_post_nms]\n",
    "        roi = roi[keep.cpu().numpy()]\n",
    "        \n",
    "        return roi \n"
   ],
   "outputs": [],
   "metadata": {
    "id": "4af7541e"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. region proposal network\n",
    "\n",
    "VGG16 통과한 feature map으로부터 region proposal들 생성"
   ],
   "metadata": {
    "id": "7a991ada"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class RegionProposalNetwork(nn.Module):\n",
    "    def __init__(self, in_channels=512, mid_channels=512, ratios=[0.5, 1, 2],\n",
    "                 anchor_scales=[8, 16, 32], feat_stride=16, proposal_creator_params=dict(),):\n",
    "        \n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "\n",
    "        self.anchor_base = generate_anchor_base(anchor_scales=anchor_scales, ratios=ratios) # 9개의 anchorbox 생성\n",
    "        self.feat_stride = feat_stride\n",
    "        self.proposal_layer = ProposalCreator(self, **proposal_creator_params) # proposal_creator_params : 해당 네트워크가 training인지 testing인지 알려준다.\n",
    "        n_anchor = self.anchor_base.shape[0] # anchor 개수\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n",
    "        self.score = nn.Conv2d(mid_channels, n_anchor * 2, 1, 1, 0)  # 9*2\n",
    "        self.loc = nn.Conv2d(mid_channels, n_anchor * 4, 1, 1, 0)   # 9*4\n",
    "        normal_init(self.conv1, 0, 0.01) # weight initalizer\n",
    "        normal_init(self.score, 0, 0.01) # weight initalizer\n",
    "        normal_init(self.loc, 0, 0.01)   # weight initalizer\n",
    "\n",
    "    def forward(self, x, img_size, scale=1.):\n",
    "        # x(feature map)\n",
    "        n, _, hh, ww = x.shape\n",
    "\n",
    "        # 전체 (h*w*9)개 anchor의 좌표값 # anchor_base:(9, 4)\n",
    "        anchor = _enumerate_shifted_anchor(np.array(self.anchor_base), self.feat_stride, hh, ww) \n",
    "        n_anchor = anchor.shape[0] // (hh * ww) # anchor 개수\n",
    "        \n",
    "        middle = F.relu(self.conv1(x))\n",
    "        \n",
    "        # predicted bounding box offset\n",
    "        rpn_locs = self.loc(middle)\n",
    "        rpn_locs = rpn_locs.permute(0, 2, 3, 1).contiguous().view(n, -1, 4) \n",
    "\n",
    "        # predicted scores for anchor (foreground or background)\n",
    "        rpn_scores = self.score(middle)  \n",
    "        rpn_scores = rpn_scores.permute(0, 2, 3, 1).contiguous() \n",
    "        \n",
    "        # scores for foreground\n",
    "        rpn_softmax_scores = F.softmax(rpn_scores.view(n, hh, ww, n_anchor, 2), dim=4) \n",
    "        rpn_fg_scores = rpn_softmax_scores[:, :, :, :, 1].contiguous()    \n",
    "        rpn_fg_scores = rpn_fg_scores.view(n, -1)    \n",
    "        \n",
    "        rpn_scores = rpn_scores.view(n, -1, 2) \n",
    "\n",
    "        # proposal생성 (ProposalCreator)\n",
    "        rois = list()        # proposal의 좌표값이 있는 bounding box array\n",
    "        roi_indices = list() # roi에 해당하는 image 인덱스\n",
    "        for i in range(n):\n",
    "            roi = self.proposal_layer(rpn_locs[i].cpu().data.numpy(),rpn_fg_scores[i].cpu().data.numpy(),anchor, img_size,scale=scale) \n",
    "            batch_index = i * np.ones((len(roi),), dtype=np.int32)\n",
    "            rois.append(roi)\n",
    "            roi_indices.append(batch_index)\n",
    "        rois = np.concatenate(rois, axis=0)\n",
    "        roi_indices = np.concatenate(roi_indices, axis=0)\n",
    "        \n",
    "        return rpn_locs, rpn_scores, rois, roi_indices, anchor\n",
    "\n",
    "\n",
    "def _enumerate_shifted_anchor(anchor_base, feat_stride, height, width):\n",
    "    # anchor_base는 하나의 pixel에 9개 종류의 anchor box를 나타냄\n",
    "    # 이것을 enumerate시켜 전체 이미지의 pixel에 각각 9개의 anchor box를 가지게 함\n",
    "    # 32x32 feature map에서는 32x32x9=9216개 anchor box가짐\n",
    "\n",
    "    shift_y = np.arange(0, height * feat_stride, feat_stride)\n",
    "    shift_x = np.arange(0, width * feat_stride, feat_stride)\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "    shift = np.stack((shift_y.ravel(), shift_x.ravel(),\n",
    "                      shift_y.ravel(), shift_x.ravel()), axis=1)\n",
    "\n",
    "    A = anchor_base.shape[0]\n",
    "    K = shift.shape[0]\n",
    "    anchor = anchor_base.reshape((1, A, 4)) + \\\n",
    "             shift.reshape((1, K, 4)).transpose((1, 0, 2))\n",
    "    anchor = anchor.reshape((K * A, 4)).astype(np.float32)\n",
    "    return anchor # (9216, 4)\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "cb749c00"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature extractor(VGG) 정의\n"
   ],
   "metadata": {
    "id": "4e1ff0bd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def decom_vgg16():\n",
    "    # the 30th layer of features is relu of conv5_3\n",
    "    model = vgg16(pretrained=True)\n",
    "#     model = vgg16()\n",
    "#     model.load_state_dict(torch.load('./checkpoints/vgg16-397923af.pth'))\n",
    "    \n",
    "    features = list(model.features)[:30]\n",
    "    classifier = model.classifier\n",
    "\n",
    "    classifier = list(classifier)\n",
    "    del classifier[6]\n",
    "    if not use_drop:\n",
    "        del classifier[5]\n",
    "        del classifier[2]\n",
    "    classifier = nn.Sequential(*classifier)\n",
    "\n",
    "    # freeze top4 conv\n",
    "    for layer in features[:10]:\n",
    "        for p in layer.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    return nn.Sequential(*features), classifier"
   ],
   "outputs": [],
   "metadata": {
    "id": "9911df3f"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Faster R-CNN head 정의\n",
    "\n",
    "RoI pool 후에 classifier, regressor 통과"
   ],
   "metadata": {
    "id": "c1cb8f86"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class VGG16RoIHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Faster R-CNN head\n",
    "    RoI pool 후에 classifier, regressior 통과\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_class, roi_size, spatial_scale, classifier):\n",
    "        super(VGG16RoIHead, self).__init__()\n",
    "\n",
    "        self.classifier = classifier  \n",
    "        self.cls_loc = nn.Linear(4096, n_class * 4) # bounding box regressor\n",
    "        self.score = nn.Linear(4096, n_class) # Classifier\n",
    "\n",
    "        normal_init(self.cls_loc, 0, 0.001)  # weight initialize\n",
    "        normal_init(self.score, 0, 0.01)     # weight initialize\n",
    "\n",
    "        self.n_class = n_class # 배경 포함한 class 수\n",
    "        self.roi_size = roi_size # RoI-pooling 후 feature map의  높이, 너비\n",
    "        self.spatial_scale = spatial_scale # roi resize scale\n",
    "        self.roi = RoIPool( (self.roi_size, self.roi_size),self.spatial_scale)\n",
    "\n",
    "    def forward(self, x, rois, roi_indices):\n",
    "        # in case roi_indices is  ndarray\n",
    "        roi_indices = totensor(roi_indices).float()\n",
    "        rois = totensor(rois).float()\n",
    "        indices_and_rois = torch.cat([roi_indices[:, None], rois], dim=1)\n",
    "        # NOTE: important: yx->xy\n",
    "        xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]\n",
    "        indices_and_rois =  xy_indices_and_rois.contiguous() \n",
    "\n",
    "        # 각 이미지 roi pooling \n",
    "        pool = self.roi(x, indices_and_rois) \n",
    "        # flatten \n",
    "        pool = pool.view(pool.size(0), -1)\n",
    "        # fully connected\n",
    "        fc7 = self.classifier(pool)\n",
    "        # regression \n",
    "        roi_cls_locs = self.cls_loc(fc7)\n",
    "        # softmax\n",
    "        roi_scores = self.score(fc7)\n",
    "\n",
    "        \n",
    "        return roi_cls_locs, roi_scores"
   ],
   "outputs": [],
   "metadata": {
    "id": "ce8ceb71"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Faster R-CNN 정의\n",
    "Feature Extraction : image로부터 feature map 생성\\\n",
    "Region Proposal Networks : Region of Interest 생성\\\n",
    "Localization and Classification Head : RoI에 해당하는 feature map을 최종 detect"
   ],
   "metadata": {
    "id": "85cec189"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def nograd(f):\n",
    "    def new_f(*args, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            return f(*args, **kwargs)\n",
    "    return new_f\n",
    "\n",
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self, extractor, rpn, head,\n",
    "                loc_normalize_mean = (0., 0., 0., 0.),\n",
    "                loc_normalize_std = (0.1, 0.1, 0.2, 0.2)):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "        self.extractor = extractor  # extractor : vgg\n",
    "        self.rpn = rpn              # rpn : region proposal network\n",
    "        self.head = head            # head : RoiHead\n",
    "\n",
    "        # mean and std\n",
    "        self.loc_normalize_mean = loc_normalize_mean\n",
    "        self.loc_normalize_std = loc_normalize_std\n",
    "        self.use_preset()\n",
    "\n",
    "    @property\n",
    "    def n_class(self): # 최종 class 개수 (배경 포함)\n",
    "        return self.head.n_class\n",
    "        \n",
    "    # predict 시 사용하는 forward\n",
    "    # train 시 FasterRCNNTrainer을 사용하여 FasterRcnn에 있는 extractor, rpn, head를 모듈별로 불러와서 forward\n",
    "    def forward(self, x, scale=1.):\n",
    "        img_size = x.shape[2:]\n",
    "\n",
    "        h = self.extractor(x) # extractor 통과\n",
    "        rpn_locs, rpn_scores, rois, roi_indices, anchor = self.rpn(h, img_size, scale) # rpn 통과\n",
    "        roi_cls_locs, roi_scores = self.head(h, rois, roi_indices) # head 통과\n",
    "        return roi_cls_locs, roi_scores, rois, roi_indices \n",
    "\n",
    "    def use_preset(self): # prediction 과정 쓰이는 threshold 정의\n",
    "        self.nms_thresh = 0.3\n",
    "        self.score_thresh = 0.05\n",
    "\n",
    "    def _suppress(self, raw_cls_bbox, raw_prob):\n",
    "        bbox = list()\n",
    "        label = list()\n",
    "        score = list()\n",
    "        \n",
    "        # skip cls_id = 0 because it is the background class\n",
    "        for l in range(1, self.n_class):\n",
    "            cls_bbox_l = raw_cls_bbox.reshape((-1, self.n_class, 4))[:, l, :]\n",
    "            prob_l = raw_prob[:, l]\n",
    "            mask = prob_l > self.score_thresh\n",
    "            cls_bbox_l = cls_bbox_l[mask]\n",
    "            prob_l = prob_l[mask]\n",
    "            keep = nms(cls_bbox_l, prob_l,self.nms_thresh)\n",
    "            bbox.append(cls_bbox_l[keep].cpu().numpy())\n",
    "            # The labels are in [0, self.n_class - 2].\n",
    "            label.append((l - 1) * np.ones((len(keep),)))\n",
    "            score.append(prob_l[keep].cpu().numpy())\n",
    "        \n",
    "        bbox = np.concatenate(bbox, axis=0).astype(np.float32)\n",
    "        label = np.concatenate(label, axis=0).astype(np.int32)\n",
    "        score = np.concatenate(score, axis=0).astype(np.float32)\n",
    "        return bbox, label, score\n",
    "\n",
    "    @nograd\n",
    "    def predict(self, imgs,sizes=None):\n",
    "        \"\"\"\n",
    "        이미지에서 객체 검출\n",
    "        Input : images\n",
    "        Output : bboxes, labels, scores\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        prepared_imgs = imgs\n",
    "                \n",
    "        bboxes = list()\n",
    "        labels = list()\n",
    "        scores = list()\n",
    "        for img, size in zip(prepared_imgs, sizes):\n",
    "            img = totensor(img[None]).float()\n",
    "            scale = img.shape[3] / size[1]\n",
    "            roi_cls_loc, roi_scores, rois, _ = self(img, scale=scale) # self = FasterRCNN\n",
    "            # We are assuming that batch size is 1.\n",
    "            roi_score = roi_scores.data\n",
    "            roi_cls_loc = roi_cls_loc.data\n",
    "            roi = totensor(rois) / scale\n",
    "\n",
    "            # Convert predictions to bounding boxes in image coordinates.\n",
    "            # Bounding boxes are scaled to the scale of the input images.\n",
    "            mean = torch.Tensor(self.loc_normalize_mean).cuda(). repeat(self.n_class)[None]\n",
    "            std = torch.Tensor(self.loc_normalize_std).cuda(). repeat(self.n_class)[None]\n",
    "\n",
    "            roi_cls_loc = (roi_cls_loc * std + mean)\n",
    "            roi_cls_loc = roi_cls_loc.view(-1, self.n_class, 4)\n",
    "            roi = roi.view(-1, 1, 4).expand_as(roi_cls_loc)\n",
    "            cls_bbox = loc2bbox(tonumpy(roi).reshape((-1, 4)),tonumpy(roi_cls_loc).reshape((-1, 4)))\n",
    "            cls_bbox = totensor(cls_bbox)\n",
    "            cls_bbox = cls_bbox.view(-1, self.n_class * 4)\n",
    "            # clip bounding box\n",
    "            cls_bbox[:, 0::2] = (cls_bbox[:, 0::2]).clamp(min=0, max=size[0])\n",
    "            cls_bbox[:, 1::2] = (cls_bbox[:, 1::2]).clamp(min=0, max=size[1])\n",
    "\n",
    "            prob = (F.softmax(totensor(roi_score), dim=1))\n",
    "\n",
    "            bbox, label, score = self._suppress(cls_bbox, prob)\n",
    "            bboxes.append(bbox)\n",
    "            labels.append(label)\n",
    "            scores.append(score)\n",
    "\n",
    "        self.use_preset()\n",
    "        self.train()\n",
    "        return bboxes, labels, scores\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        '''\n",
    "        Optimizer 선언\n",
    "        '''\n",
    "        lr = learning_rate\n",
    "        params = []\n",
    "        for key, value in dict(self.named_parameters()).items():\n",
    "            if value.requires_grad:\n",
    "                if 'bias' in key:\n",
    "                    params += [{'params': [value], 'lr': lr * 2, 'weight_decay': 0}]\n",
    "                else:\n",
    "                    params += [{'params': [value], 'lr': lr, 'weight_decay': weight_decay}]\n",
    "        self.optimizer = torch.optim.SGD(params, momentum=0.9)\n",
    "        return self.optimizer\n",
    "\n",
    "    def scale_lr(self, decay=0.1):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] *= decay\n",
    "        return self.optimizer\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "0a04aba6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Faster R-CNN 생성\n",
    "Extractor(VGG) + RPN + Head 합치기"
   ],
   "metadata": {
    "id": "39849b21"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class FasterRCNNVGG16(FasterRCNN):\n",
    "\n",
    "    feat_stride = 16  # downsample 16x for output of conv5 in vgg16\n",
    "\n",
    "    def __init__(self, n_fg_class=10, ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32] ): # n_fg_class : 배경포함 하지 않은 class 개수        \n",
    "        extractor, classifier = decom_vgg16()\n",
    "        \n",
    "        rpn = RegionProposalNetwork(\n",
    "            512, 512,\n",
    "            ratios=ratios,\n",
    "            anchor_scales=anchor_scales,\n",
    "            feat_stride=self.feat_stride,\n",
    "        )\n",
    "\n",
    "        head = VGG16RoIHead(\n",
    "            n_class=n_fg_class + 1,\n",
    "            roi_size=7,\n",
    "            spatial_scale=(1. / self.feat_stride),\n",
    "            classifier=classifier\n",
    "        )\n",
    "        super(FasterRCNNVGG16, self).__init__(\n",
    "            extractor,\n",
    "            rpn,\n",
    "            head,\n",
    "        )\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "32726880"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trainer "
   ],
   "metadata": {
    "id": "46202f13"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 0. util 함수 정의\n",
    "bouningb box IoU"
   ],
   "metadata": {
    "id": "61acb16a"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def bbox_iou(bbox_a, bbox_b):\n",
    "    if bbox_a.shape[1] != 4 or bbox_b.shape[1] != 4:\n",
    "        raise IndexError\n",
    "\n",
    "    #bbox_a 1개와 bbox_b k개를 비교해야하므로 None을 이용해서 차원을 늘려서 연산한다.\n",
    "    # top left\n",
    "    tl = np.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n",
    "    # bottom right\n",
    "    br = np.minimum(bbox_a[:, None, 2:], bbox_b[:, 2:])\n",
    "\n",
    "    area_i = np.prod(br - tl, axis=2) * (tl < br).all(axis=2)\n",
    "    area_a = np.prod(bbox_a[:, 2:] - bbox_a[:, :2], axis=1)\n",
    "    area_b = np.prod(bbox_b[:, 2:] - bbox_b[:, :2], axis=1)\n",
    "    return area_i / (area_a[:, None] + area_b - area_i)"
   ],
   "outputs": [],
   "metadata": {
    "id": "c9d25d7e"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Anchor Target Creator\n",
    "Anchor box에 해당하는 ground truth bounding box match\\\n",
    "Region Proposal Network loss 구할 때 ground truth로 사용"
   ],
   "metadata": {
    "id": "ac3ce234"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "class AnchorTargetCreator(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_sample=256,\n",
    "                 pos_iou_thresh=0.7, neg_iou_thresh=0.3,\n",
    "                 pos_ratio=0.5):\n",
    "        self.n_sample = n_sample\n",
    "        self.pos_iou_thresh = pos_iou_thresh\n",
    "        self.neg_iou_thresh = neg_iou_thresh\n",
    "        self.pos_ratio = pos_ratio\n",
    "\n",
    "    def __call__(self, bbox, anchor, img_size):\n",
    "\n",
    "        img_H, img_W = img_size\n",
    "\n",
    "        n_anchor = len(anchor) # 9216\n",
    "        inside_index = get_inside_index(anchor, img_H, img_W) # (2272,)\n",
    "        anchor = anchor[inside_index] # (2272, 4)\n",
    "        argmax_ious, label = self._create_label(\n",
    "            inside_index, anchor, bbox)\n",
    "\n",
    "        # compute bounding box regression targets\n",
    "        loc = bbox2loc(anchor, bbox[argmax_ious]) # (2272, 4)\n",
    "\n",
    "        # map up to original set of anchors\n",
    "        label = unmap(label, n_anchor, inside_index, fill=-1) # (9216,)\n",
    "        loc = unmap(loc, n_anchor, inside_index, fill=0) # (9216, 4)\n",
    "\n",
    "        return loc, label\n",
    "\n",
    "    def _create_label(self, inside_index, anchor, bbox):\n",
    "        # label) 1 :positive, 0 : negative, -1 : dont care\n",
    "        label = np.empty((len(inside_index),), dtype=np.int32)\n",
    "        label.fill(-1)\n",
    "\n",
    "        argmax_ious, max_ious, gt_argmax_ious = self._calc_ious(anchor, bbox, inside_index)\n",
    "\n",
    "        label[max_ious < self.neg_iou_thresh] = 0 # 0.3\n",
    "\n",
    "        # 가장 iou가 큰 것은 positive label\n",
    "        label[gt_argmax_ious] = 1\n",
    "\n",
    "        # positive label\n",
    "        label[max_ious >= self.pos_iou_thresh] = 1 # 0.7\n",
    "\n",
    "        # subsample positive labels if we have too many\n",
    "        n_pos = int(self.pos_ratio * self.n_sample)\n",
    "        pos_index = np.where(label == 1)[0]\n",
    "        if len(pos_index) > n_pos:\n",
    "            disable_index = np.random.choice(\n",
    "                pos_index, size=(len(pos_index) - n_pos), replace=False)\n",
    "            label[disable_index] = -1\n",
    "\n",
    "        # subsample negative labels if we have too many\n",
    "        n_neg = self.n_sample - np.sum(label == 1)\n",
    "        neg_index = np.where(label == 0)[0]\n",
    "        if len(neg_index) > n_neg:\n",
    "            disable_index = np.random.choice(\n",
    "                neg_index, size=(len(neg_index) - n_neg), replace=False)\n",
    "            label[disable_index] = -1\n",
    "\n",
    "        return argmax_ious, label\n",
    "\n",
    "    def _calc_ious(self, anchor, bbox, inside_index):\n",
    "        # ious between the anchors and the gt boxes\n",
    "        ious = bbox_iou(anchor, bbox)\n",
    "        argmax_ious = ious.argmax(axis=1)\n",
    "        max_ious = ious[np.arange(len(inside_index)), argmax_ious]\n",
    "        gt_argmax_ious = ious.argmax(axis=0)\n",
    "        gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n",
    "        gt_argmax_ious = np.where(ious == gt_max_ious)[0]\n",
    "\n",
    "        return argmax_ious, max_ious, gt_argmax_ious"
   ],
   "outputs": [],
   "metadata": {
    "id": "69a1e360"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. positive, negative sampling\n",
    "RPN에서 NMS를 거친 roi들을 ground truth와의 iou를 비교\\\n",
    "positive / negative sampling 수행 (총 128개)\\\n",
    "sample roi와 gt_bbox를 이용해 bbox regression에서 regression해야할 ground truth loc값(t_x, t_y, t_w, t_h)을 구함"
   ],
   "metadata": {
    "id": "9a11881f"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "class ProposalTargetCreator:\n",
    "    def __init__(self,\n",
    "                 n_sample=128,\n",
    "                 pos_ratio=0.25, pos_iou_thresh=0.5,\n",
    "                 neg_iou_thresh_hi=0.5, neg_iou_thresh_lo=0.0\n",
    "                 ):\n",
    "        self.n_sample = n_sample\n",
    "        self.pos_ratio = pos_ratio\n",
    "        self.pos_iou_thresh = pos_iou_thresh # positive iou threshold\n",
    "        self.neg_iou_thresh_hi = neg_iou_thresh_hi # negitave iou threshold = (neg_iou_thresh_hi ~ neg_iou_thresh_lo)\n",
    "        self.neg_iou_thresh_lo = neg_iou_thresh_lo \n",
    "\n",
    "    def __call__(self, roi, bbox, label,\n",
    "                 loc_normalize_mean=(0., 0., 0., 0.),\n",
    "                 loc_normalize_std=(0.1, 0.1, 0.2, 0.2)):\n",
    "        n_bbox, _ = bbox.shape\n",
    "\n",
    "        roi = np.concatenate((roi, bbox), axis=0)\n",
    "\n",
    "        pos_roi_per_image = np.round(self.n_sample * self.pos_ratio) # positive image 갯수 = 32\n",
    "        iou = bbox_iou(roi, bbox) # RoI와 bounding box IoU\n",
    "        gt_assignment = iou.argmax(axis=1)\n",
    "        max_iou = iou.max(axis=1)\n",
    "        gt_roi_label = label[gt_assignment] + 1 # class label [0, n_fg_class - 1] -> [1, n_fg_class].\n",
    "\n",
    "        # positive sample 선택 (>= pos_iou_thresh IoU)\n",
    "        pos_index = np.where(max_iou >= self.pos_iou_thresh)[0]\n",
    "        pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))\n",
    "        if pos_index.size > 0:\n",
    "            pos_index = np.random.choice(\n",
    "                pos_index, size=pos_roi_per_this_image, replace=False)\n",
    "\n",
    "        # Negative sample 선택 [neg_iou_thresh_lo, neg_iou_thresh_hi)\n",
    "        neg_index = np.where((max_iou < self.neg_iou_thresh_hi) &\n",
    "                             (max_iou >= self.neg_iou_thresh_lo))[0]\n",
    "        neg_roi_per_this_image = self.n_sample - pos_roi_per_this_image\n",
    "        neg_roi_per_this_image = int(min(neg_roi_per_this_image,\n",
    "                                         neg_index.size))\n",
    "        if neg_index.size > 0:\n",
    "            neg_index = np.random.choice(\n",
    "                neg_index, size=neg_roi_per_this_image, replace=False)\n",
    "\n",
    "        # The indices that we're selecting (both positive and negative).\n",
    "        keep_index = np.append(pos_index, neg_index)\n",
    "        gt_roi_label = gt_roi_label[keep_index]\n",
    "        gt_roi_label[pos_roi_per_this_image:] = 0  # negative sample의 label = 0\n",
    "        sample_roi = roi[keep_index] # (128, 4)\n",
    "\n",
    "        # sample roi와 gt_bbox를 이용해 bbox regression에서 regression해야할 ground truth loc값(t_x, t_y, t_w, t_h) 계산\n",
    "        gt_roi_loc = bbox2loc(sample_roi, bbox[gt_assignment[keep_index]]) # (128, 4)\n",
    "        gt_roi_loc = ((gt_roi_loc - np.array(loc_normalize_mean, np.float32)) / np.array(loc_normalize_std, np.float32))\n",
    "\n",
    "        return sample_roi, gt_roi_loc, gt_roi_label"
   ],
   "outputs": [],
   "metadata": {
    "id": "e6928b27"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Trainer 정의\n",
    "training, loss 계산, checkpoint 저장 및 불러오기"
   ],
   "metadata": {
    "id": "ec2da581"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "LossTuple = namedtuple('LossTuple', ['rpn_loc_loss', 'rpn_cls_loss',\n",
    "                                     'roi_loc_loss', 'roi_cls_loss',\n",
    "                                     'total_loss'])\n",
    "class FasterRCNNTrainer(nn.Module):\n",
    "\n",
    "    def __init__(self, faster_rcnn):\n",
    "        super(FasterRCNNTrainer, self).__init__()\n",
    "\n",
    "        self.faster_rcnn = faster_rcnn\n",
    "        self.rpn_sigma = rpn_sigma\n",
    "        self.roi_sigma = roi_sigma\n",
    "\n",
    "        # target creator create gt_bbox gt_label etc as training targets. \n",
    "        self.anchor_target_creator = AnchorTargetCreator()\n",
    "        self.proposal_target_creator = ProposalTargetCreator()\n",
    "\n",
    "        self.loc_normalize_mean = faster_rcnn.loc_normalize_mean\n",
    "        self.loc_normalize_std = faster_rcnn.loc_normalize_std\n",
    "\n",
    "        self.optimizer = self.faster_rcnn.get_optimizer()\n",
    "\n",
    "        # training 상태 보여주는 지표\n",
    "        self.rpn_cm = ConfusionMeter(2) # confusion matrix for classification\n",
    "        self.roi_cm = ConfusionMeter(11)  # confusion matrix for classification\n",
    "        self.meters = {k: AverageValueMeter() for k in LossTuple._fields}  # average loss\n",
    "\n",
    "    def forward(self, imgs, bboxes, labels, scale):\n",
    "        n = bboxes.shape[0]\n",
    "        \n",
    "        if n != 1:\n",
    "            raise ValueError('Currently only batch size 1 is supported.')\n",
    "\n",
    "        _, _, H, W = imgs.shape\n",
    "        img_size = (H, W)\n",
    "\n",
    "        # VGG (features extractor)\n",
    "        features = self.faster_rcnn.extractor(imgs)\n",
    "        \n",
    "        # RPN (region proposal)\n",
    "        rpn_locs, rpn_scores, rois, roi_indices, anchor = self.faster_rcnn.rpn(features, img_size, scale)\n",
    "\n",
    "        # Since batch size is one, convert variables to singular form\n",
    "        bbox = bboxes[0]\n",
    "        label = labels[0]\n",
    "        rpn_score = rpn_scores[0]\n",
    "        rpn_loc = rpn_locs[0]\n",
    "        roi = rois\n",
    "\n",
    "        \"\"\"\n",
    "        sample roi =  rpn에서 nms 거친 2000개의 roi들 중 positive/negative 비율 고려해 최종 sampling한 roi\n",
    "        \"\"\"\n",
    "        sample_roi, gt_roi_loc, gt_roi_label = self.proposal_target_creator(\n",
    "            roi,\n",
    "            tonumpy(bbox),\n",
    "            tonumpy(label),\n",
    "            self.loc_normalize_mean,\n",
    "            self.loc_normalize_std)\n",
    "        \n",
    "        # NOTE it's all zero because now it only support for batch=1 now\n",
    "        # Faster R-CNN head (prediction head)\n",
    "        sample_roi_index = torch.zeros(len(sample_roi))\n",
    "        roi_cls_loc, roi_score = self.faster_rcnn.head(features,sample_roi,sample_roi_index) \n",
    "\n",
    "        # ------------------ RPN losses -------------------#\n",
    "        gt_rpn_loc, gt_rpn_label = self.anchor_target_creator(tonumpy(bbox),anchor,img_size) \n",
    "        gt_rpn_label = totensor(gt_rpn_label).long() \n",
    "        gt_rpn_loc = totensor(gt_rpn_loc) \n",
    "        \n",
    "        # rpn bounding box regression loss\n",
    "        rpn_loc_loss = _fast_rcnn_loc_loss(rpn_loc,gt_rpn_loc,gt_rpn_label.data,self.rpn_sigma)\n",
    "        # rpn classification loss\n",
    "        rpn_cls_loss = F.cross_entropy(rpn_score, gt_rpn_label.cuda(), ignore_index=-1)\n",
    "        \n",
    "        _gt_rpn_label = gt_rpn_label[gt_rpn_label > -1]\n",
    "        _rpn_score = tonumpy(rpn_score)[tonumpy(gt_rpn_label) > -1]\n",
    "        self.rpn_cm.add(totensor(_rpn_score, False), _gt_rpn_label.data.long())\n",
    "\n",
    "        # ------------------ ROI losses (fast rcnn loss) -------------------#\n",
    "        n_sample = roi_cls_loc.shape[0] \n",
    "        roi_cls_loc = roi_cls_loc.view(n_sample, -1, 4)\n",
    "        roi_loc = roi_cls_loc[torch.arange(0, n_sample).long().cuda(), \\\n",
    "                              totensor(gt_roi_label).long()]\n",
    "        gt_roi_label = totensor(gt_roi_label).long() \n",
    "        gt_roi_loc = totensor(gt_roi_loc) \n",
    "\n",
    "        # faster rcnn bounding box regression loss\n",
    "        roi_loc_loss = _fast_rcnn_loc_loss(\n",
    "            roi_loc.contiguous(),\n",
    "            gt_roi_loc,\n",
    "            gt_roi_label.data,\n",
    "            self.roi_sigma)\n",
    "\n",
    "        # faster rcnn classification loss\n",
    "        roi_cls_loss = nn.CrossEntropyLoss()(roi_score, gt_roi_label.cuda())\n",
    "        \n",
    "        self.roi_cm.add(totensor(roi_score, False), gt_roi_label.data.long())\n",
    "\n",
    "        losses = [rpn_loc_loss, rpn_cls_loss, roi_loc_loss, roi_cls_loss]\n",
    "        losses = losses + [sum(losses)] # total_loss == sum(losses)\n",
    "\n",
    "        return LossTuple(*losses)\n",
    "    \n",
    "    # training\n",
    "    def train_step(self, imgs, bboxes, labels, scale):\n",
    "        self.optimizer.zero_grad()\n",
    "        losses = self.forward(imgs, bboxes, labels, scale)\n",
    "        losses.total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.update_meters(losses)\n",
    "        return losses\n",
    "    \n",
    "    # checkpoint 만들기\n",
    "    def save(self, save_optimizer=False, save_path=None):\n",
    "        save_dict = dict()\n",
    "\n",
    "        save_dict['model'] = self.faster_rcnn.state_dict()\n",
    "\n",
    "        if save_optimizer:\n",
    "            save_dict['optimizer'] = self.optimizer.state_dict()\n",
    "\n",
    "        if save_path is None:\n",
    "            save_path = './checkpoints/faster_rcnn_scratch_checkpoints.pth'\n",
    "\n",
    "        save_dir = os.path.dirname(save_path)\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        torch.save(save_dict, save_path)\n",
    "        return save_path\n",
    "    \n",
    "    # checkpoint load\n",
    "    def load(self, path, load_optimizer=True, parse_opt=False, ):\n",
    "        state_dict = torch.load(path)\n",
    "        if 'model' in state_dict:\n",
    "            self.faster_rcnn.load_state_dict(state_dict['model'])\n",
    "        else:  # legacy way, for backward compatibility\n",
    "            self.faster_rcnn.load_state_dict(state_dict)\n",
    "            return self\n",
    "        if 'optimizer' in state_dict and load_optimizer:\n",
    "            self.optimizer.load_state_dict(state_dict['optimizer'])\n",
    "        return self\n",
    "\n",
    "    def update_meters(self, losses):\n",
    "        loss_d = {k: scalar(v) for k, v in losses._asdict().items()}\n",
    "        for key, meter in self.meters.items():\n",
    "            meter.add(loss_d[key])\n",
    "\n",
    "    def reset_meters(self):\n",
    "        for key, meter in self.meters.items():\n",
    "            meter.reset()\n",
    "        self.roi_cm.reset()\n",
    "        self.rpn_cm.reset()\n",
    "\n",
    "    def get_meter_data(self):\n",
    "        return {k: v.value()[0] for k, v in self.meters.items()}\n",
    "\n",
    "\n",
    "def _smooth_l1_loss(x, t, in_weight, sigma):\n",
    "    sigma2 = sigma ** 2\n",
    "    diff = in_weight * (x - t)\n",
    "    abs_diff = diff.abs()\n",
    "    flag = (abs_diff.data < (1. / sigma2)).float()\n",
    "    y = (flag * (sigma2 / 2.) * (diff ** 2) +\n",
    "         (1 - flag) * (abs_diff - 0.5 / sigma2))\n",
    "    return y.sum()\n",
    "\n",
    "\n",
    "def _fast_rcnn_loc_loss(pred_loc, gt_loc, gt_label, sigma):\n",
    "    # Localization loss 구할 때는 positive example에 대해서만 계산\n",
    "    in_weight = torch.zeros(gt_loc.shape).cuda()\n",
    "    in_weight[(gt_label > 0).view(-1, 1).expand_as(in_weight).cuda()] = 1\n",
    "    loc_loss = _smooth_l1_loss(pred_loc, gt_loc, in_weight.detach(), sigma)\n",
    "    loc_loss /= ((gt_label >= 0).sum().float())\n",
    "    return loc_loss"
   ],
   "outputs": [],
   "metadata": {
    "id": "d40ccd0b"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train"
   ],
   "metadata": {
    "id": "2b03eee6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def train():\n",
    "    # Train dataset 불러오기\n",
    "#     dataset = TrainDataset()\n",
    "    annotation = os.path.join(data_dir,'train.json')\n",
    "    dataset = TrainCustom(annotation, data_dir, transforms=True)\n",
    "    print('load data')\n",
    "    dataloader = data_.DataLoader(dataset, \n",
    "                                  batch_size=1,     # only batch_size=1 support\n",
    "                                  shuffle=True, \n",
    "                                  pin_memory=False,\n",
    "                                  num_workers=1)\n",
    "    \n",
    "    # faster rcnn 불러오기\n",
    "    faster_rcnn = FasterRCNNVGG16().cuda()\n",
    "    print('model construct completed')\n",
    "    \n",
    "    # faster rcnn trainer 불러오기\n",
    "    trainer = FasterRCNNTrainer(faster_rcnn).cuda()\n",
    "    \n",
    "    # checkpoint load\n",
    "    if train_load_path:\n",
    "        trainer.load(train_load_path)\n",
    "        print('load pretrained model from %s' % train_load_path)\n",
    "    \n",
    "    lr_ = learning_rate\n",
    "    best_loss = 1000\n",
    "    for epoch in range(epochs):\n",
    "        trainer.reset_meters()\n",
    "        for ii, (img, bbox_, label_, scale) in enumerate(tqdm(dataloader)):\n",
    "            \n",
    "            img, bbox, label = img.cuda().float(), bbox_.cuda(), label_.cuda()\n",
    "            trainer.train_step(img, bbox, label, float(scale))\n",
    "        \n",
    "        losses = trainer.get_meter_data()\n",
    "        print(f\"Epoch #{epoch+1} loss: {losses}\")\n",
    "        if losses['total_loss'] < best_loss :\n",
    "            trainer.save()\n",
    "            \n",
    "        if epoch == 9:\n",
    "            trainer.faster_rcnn.scale_lr(lr_decay)\n",
    "            lr_ = lr_ * lr_decay\n",
    "\n",
    "        # if epoch == 12: # 변경\n",
    "        #     break"
   ],
   "outputs": [],
   "metadata": {
    "id": "9c9224c3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "train()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.17s)\n",
      "creating index...\n",
      "index created!\n",
      "load data\n",
      "model construct completed\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4883/4883 [10:40<00:00,  7.63it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch #1 loss: {'rpn_loc_loss': 0.16438589899251424, 'rpn_cls_loss': 0.305231318428591, 'roi_loc_loss': 0.29827395409125884, 'roi_cls_loss': 0.5844877451168475, 'total_loss': 1.3523789170504652}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4883/4883 [10:33<00:00,  7.70it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch #2 loss: {'rpn_loc_loss': 0.15409794075926578, 'rpn_cls_loss': 0.2532035686325916, 'roi_loc_loss': 0.26182254641753944, 'roi_cls_loss': 0.5467349399142852, 'total_loss': 1.2158589953987349}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4883/4883 [10:38<00:00,  7.64it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch #3 loss: {'rpn_loc_loss': 0.15212481622245352, 'rpn_cls_loss': 0.23655230605781416, 'roi_loc_loss': 0.24787258337933082, 'roi_cls_loss': 0.5295690621100426, 'total_loss': 1.1661187672276945}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4883/4883 [10:38<00:00,  7.65it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch #4 loss: {'rpn_loc_loss': 0.14900946561397677, 'rpn_cls_loss': 0.23235997706731795, 'roi_loc_loss': 0.23648399482369548, 'roi_cls_loss': 0.5177076320455157, 'total_loss': 1.1355610700744223}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4883/4883 [10:38<00:00,  7.65it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch #5 loss: {'rpn_loc_loss': 0.14720413585149977, 'rpn_cls_loss': 0.22421661895346437, 'roi_loc_loss': 0.23013928033106434, 'roi_cls_loss': 0.5065559122342184, 'total_loss': 1.1081159475099058}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4883/4883 [10:39<00:00,  7.64it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch #6 loss: {'rpn_loc_loss': 0.14632494205512667, 'rpn_cls_loss': 0.21996174075419667, 'roi_loc_loss': 0.2225521692684858, 'roi_cls_loss': 0.49821235860629004, 'total_loss': 1.0870512114967859}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4883/4883 [10:43<00:00,  7.59it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch #7 loss: {'rpn_loc_loss': 0.14560345743638364, 'rpn_cls_loss': 0.2162925167546704, 'roi_loc_loss': 0.22094024003525095, 'roi_cls_loss': 0.5011542776850155, 'total_loss': 1.0839904926804245}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4883/4883 [10:38<00:00,  7.65it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch #8 loss: {'rpn_loc_loss': 0.14488979131181823, 'rpn_cls_loss': 0.21462648605861875, 'roi_loc_loss': 0.2177446521604429, 'roi_cls_loss': 0.49080507713174615, 'total_loss': 1.0680660060804261}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4883/4883 [10:44<00:00,  7.58it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch #9 loss: {'rpn_loc_loss': 0.14389140542596732, 'rpn_cls_loss': 0.2140002322178957, 'roi_loc_loss': 0.21491693443308377, 'roi_cls_loss': 0.48704916033943435, 'total_loss': 1.0598577323255036}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4883/4883 [10:41<00:00,  7.61it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch #10 loss: {'rpn_loc_loss': 0.14361515851390583, 'rpn_cls_loss': 0.21210523077788235, 'roi_loc_loss': 0.21320323346768189, 'roi_cls_loss': 0.48405405845414556, 'total_loss': 1.0529776827239379}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4883/4883 [10:39<00:00,  7.64it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch #11 loss: {'rpn_loc_loss': 0.13297207912452144, 'rpn_cls_loss': 0.1732188020371269, 'roi_loc_loss': 0.19282601452858877, 'roi_cls_loss': 0.4029554300942483, 'total_loss': 0.9019723262240353}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4883/4883 [10:40<00:00,  7.62it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch #12 loss: {'rpn_loc_loss': 0.12947996008471757, 'rpn_cls_loss': 0.1626245126509843, 'roi_loc_loss': 0.1872167414432486, 'roi_cls_loss': 0.38186767681084166, 'total_loss': 0.8611888903167033}\n"
     ]
    }
   ],
   "metadata": {
    "id": "1d704420"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inference"
   ],
   "metadata": {
    "id": "b0771b1d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def eval(dataloader, faster_rcnn):\n",
    "    outputs = []\n",
    "    for ii, (imgs, sizes) in enumerate(tqdm(dataloader)):\n",
    "        sizes = [sizes[0][0].item(), sizes[1][0].item()]\n",
    "        pred_bboxes_, pred_labels_, pred_scores_ = faster_rcnn.predict(imgs, [sizes])\n",
    "        for out in range(len(pred_bboxes_)):\n",
    "            outputs.append({'boxes':pred_bboxes_[out], 'scores': pred_scores_[out], 'labels': pred_labels_[out]})\n",
    "    \n",
    "    return outputs"
   ],
   "outputs": [],
   "metadata": {
    "id": "43a67b27"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def inference():\n",
    "\n",
    "    # Test dataset 불러오기\n",
    "#     testset = TestDataset()\n",
    "    annotation = os.path.join(data_dir,'test.json')\n",
    "    testset = TestCustom(annotation, data_dir)\n",
    "    test_dataloader = data_.DataLoader(testset,\n",
    "                                       batch_size=1, # only batch_size=1 support\n",
    "                                       num_workers=4,\n",
    "                                       shuffle=False, \n",
    "                                       pin_memory=False\n",
    "                                       )\n",
    "    # faster rcnn 불러오기\n",
    "    faster_rcnn = FasterRCNNVGG16().cuda()\n",
    "    state_dict = torch.load(inf_load_path)\n",
    "    if 'model' in state_dict:\n",
    "        faster_rcnn.load_state_dict(state_dict['model'])\n",
    "    print('load pretrained model from %s' % inf_load_path)\n",
    "\n",
    "    # evaluation\n",
    "    outputs = eval(test_dataloader, faster_rcnn)\n",
    "    score_threshold = 0.05\n",
    "    prediction_strings = []\n",
    "    file_names = []\n",
    "    \n",
    "    # submission file 작성\n",
    "    coco = COCO(os.path.join(data_dir, 'test.json'))\n",
    "    for i, output in enumerate(outputs):\n",
    "        prediction_string = ''\n",
    "        image_info = coco.loadImgs(coco.getImgIds(imgIds=i))[0]\n",
    "        for box, score, label in zip(output['boxes'], output['scores'], output['labels']):\n",
    "            if score > score_threshold:\n",
    "                prediction_string += str(label) + ' ' + str(score) + ' ' + str(box[1]) + ' ' + str(\n",
    "                    box[0]) + ' ' + str(box[3]) + ' ' + str(box[2]) + ' '\n",
    "        prediction_strings.append(prediction_string)\n",
    "        file_names.append(image_info['file_name'])\n",
    "    submission = pd.DataFrame()\n",
    "    submission['PredictionString'] = prediction_strings\n",
    "    submission['image_id'] = file_names\n",
    "    submission.to_csv(\"./faster_rcnn_scratch_submission.csv\", index=False)\n",
    "    \n",
    "    print(submission.head())        "
   ],
   "outputs": [],
   "metadata": {
    "id": "c77514fc"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "inference()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "load pretrained model from ./checkpoints/faster_rcnn_scratch_checkpoints.pth\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4871/4871 [04:55<00:00, 16.50it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "                                    PredictionString       image_id\n",
      "0  0 0.20416196 237.57047 685.58386 300.41238 745...  test/0000.jpg\n",
      "1  0 0.4229214 642.8509 647.4691 693.11346 821.25...  test/0001.jpg\n",
      "2  0 0.21700224 665.47675 228.45541 776.94574 320...  test/0002.jpg\n",
      "3  9 0.95920515 73.8111 228.25854 955.9991 813.55...  test/0003.jpg\n",
      "4  0 0.33455566 379.62598 384.2812 685.6206 754.5...  test/0004.jpg\n"
     ]
    }
   ],
   "metadata": {
    "id": "06b16e50"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reference\n",
    "https://github.com/chenyuntc/simple-faster-rcnn-pytorch \\\n",
    "https://github.com/shkim960520/faster-rcnn-for-studying "
   ],
   "metadata": {
    "id": "c4f8b59f"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Faster_rcnn_baseline_v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "detection",
   "language": "python",
   "name": "detection"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}